{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BM 336546 - HW1: Fetal Cardiotocograms \n",
    "# Part I: Data Exploration\n",
    "\n",
    "In this homework you will be working on predicting fetal outcomes from continuous labor monitoring using Cardiotography (CTG). In particular you will use measures of the fetal heart rate (FHR) and use these as input features to your linear classifier. Before we dive into the assignment itself, let's make a quick introduction to CTG and get familiar with this type of examination and its underlying physiological basis as a good biomedical engineer!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intrapartum CTG is used routinely to measure maternal uterine pressure and fetal heart rate (FHR). Antepartum CTG monitoring is used to identify fetuses at risk of intrauterine hypoxia and acidaemia. As early as 28 weeks of gestation, analysis of the FHR trace is used as a nonstress test to assess the fetal well-being. In the perinatal period, timely, appropriate intervention can avoid fetal neurological damage or death. The CTG is visually assessed by a clinician or interpreted by computer analysis. In the context of labor monitoring, the CTG is used for continuous fetal monitoring. An abnormal heart rate will lead the clinician to perform a cesarean. We will focus on CTG monitoring during labor in this assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CTG has two different transducers: One of them is a transducer placed on the mother’s abdomen, above the fetal heart, to monitor heart rate using Doppler probe (cardiogram). The other is located at the fundus of the uterus to measure frequency of contractions (tocogram). Tocodynamometry is a strain gauge technology provides contraction frequency and approximate duration of labor contractions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can extract a lot of features from a CTG. Here are some of them:\n",
    "* **Uterine activity**: Duration, frequency and intensity of contractions.\n",
    "* **Baseline FHR**: Mean FHR rounded to increments of 5 beats per minute (bpm) during a 10-minute window.\n",
    "* **Baseline FHR variability**: Fluctuations in the baseline FHR that are irregular in amplitude and frequency.\n",
    "* **Presence of accelerations**: A visually apparent abrupt increase in fetal heart rate.\n",
    "\n",
    "Here is an example of a typical CTG with some of its features:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](https://ai2-s2-public.s3.amazonaws.com/figures/2017-08-08/6ed5ef1da100ebc2241c3a3945e1da9ce79f73ac/1-Figure1-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CTG dataset is an Excel file which was sent to you. For more information, please look at the Excel sheet called Description or take a look at this [link](http://archive.ics.uci.edu/ml/datasets/Cardiotocography). Our main goal in this assignment is to train an algorithm to decide what is the fetal state according to the extracted features. Before we even start dealing with the data itself, we should apply the first and most important rule of data/signal processing: **ALWAYS LOOK AND UNDERSTAND THE DATA FIRST!**\n",
    "\n",
    "In order to do that, we will load the file into a variable called CTG_features and use descriptive statistics and visualization tools you have seen in the lectures.\n",
    "\n",
    "\n",
    "* **Important note**: Every result must have an interpretation and\\or explanation. Points will be deducted for uncommented results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This block makes sure the following cells are synchronized with the modules (.py files).\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "file = Path.cwd().joinpath('messed_CTG.xls') # concatenates messed_CTG.xls to the current folder that should be the extracted zip folder \n",
    "CTG_dataset = pd.read_excel(file, sheet_name='Raw Data')\n",
    "CTG_features = CTG_dataset[['LB', 'AC', 'FM', 'UC', 'DL', 'DS', 'DR', 'DP', 'ASTV', 'MSTV', 'ALTV', 'MLTV',\n",
    " 'Width', 'Min', 'Max', 'Nmax', 'Nzeros', 'Mode', 'Mean', 'Median', 'Variance', 'Tendency']]\n",
    "CTG_morph = CTG_dataset[['CLASS']]\n",
    "fetal_state = CTG_dataset[['NSP']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First  look at the data in your Excel file. You can see that in some of the cells we have '--' or NaN etc. Furthermore, the description tells us that the feature `DR` was removed although we did load it into our dataset. Your first job is: Implement the function `rm_ext_and_nan` in the module  `clean_data` so it will remove the extra feature `DR` (ignore the feature), and **all** non-numeric values (ignore the samples). Notice that real data can have missing values in many different ways and thus your implementation has to be generic. This function should return a dictionary of features where the values of each feature are the clean excel columns without the `DR` feature. **Hint**: In order to eliminate every cell that is non-numeric, you will have to transform it first to NaN and only then eliminate them. **Note**: `CTG_dataset` is a `pandas DataFrame` and every element within it is called `pandas series`. Implement the function in a single line of code using dictionary comprehensions.\n",
    "\n",
    "A note on debugging using PyCharm:\n",
    "* In order to fill the needed modules (i.e. the .py files functions like \"rm_ext_and_nan\" in the next block) we must open the .py files by some text editor. A simple solution would be to open it in this notebook by double-clicking on the file, but no debugging options are available. To debug the file you can open the directory of this homework as a project in PyCharm (see Moodle video), and then run the .py file in debug mode. In order to execute a function, you must prepare it's input variables beforehand. For example, in \"clean_data.py\" you will see in the \"Debugging Block!\" the same lines from the previous block (copy paste) and the next in order to run \"rm_ext_and_nan\" function successfully. To debug other functions you should do a similar procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from clean_data import rm_ext_and_nan as rm\n",
    "extra_feature = 'DR' \n",
    "c_ctg = rm(CTG_features, extra_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare and make sure that your function works well: let's compare the histograms' width feature . First we will plot the original distribution of this feature where every NaN element was replaced by a value that is not reasonable as 1000 :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "feat = 'Width'\n",
    "Q = pd.DataFrame(CTG_features[feat])\n",
    "idx_na = Q.index[Q[feat].isna() == True].tolist()\n",
    "for i in idx_na:\n",
    "    Q.loc[i] = 1000\n",
    "Q.hist(bins = 100, color='orange')\n",
    "plt.xlabel('Histogram Width')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run the following lines of code to check how you performed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat = 'Width'\n",
    "Q_clean = pd.DataFrame(c_ctg[feat])\n",
    "Q_clean.hist(bins=100)\n",
    "plt.xlabel('Histogram Width')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we are warmed up, let's do something a bit different. Instead of removing the NaN values, handle those missing values using sampling (**Tutorial C02**). Again, first convert all non-numeric values to NaN and only then apply the sampling method. Implement the function `nan2num_samp`. Don't forget to remove `DR` again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from clean_data import nan2num_samp\n",
    "extra_feature = 'DR' \n",
    "c_samp = nan2num_samp(CTG_features, extra_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run the following lines of code to check how you performed for example with the feature `MSTV`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat = 'MSTV'\n",
    "print(CTG_features[feat].iloc[0:5]) # print first 5 values\n",
    "print(c_samp[feat].iloc[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize our \"clean\" data using histograms, barplots and boxplots and then refer to the following questions below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplots\n",
    "c_samp.boxplot(column=['Median','Mean','Mode'])\n",
    "plt.ylabel('Fetal Heart Rate [bpm]')\n",
    "# Histograms\n",
    "xlbl = ['beats/min','1/sec','1/sec','%']\n",
    "axarr = c_samp.hist(column=['LB', 'AC', 'UC','ASTV'], bins=100,layout = (2, 2),figsize=(20, 10))\n",
    "for i,ax in enumerate(axarr.flatten()):\n",
    "    ax.set_xlabel(xlbl[i])\n",
    "    ax.set_ylabel(\"Count\")\n",
    "# Barplots (error bars)\n",
    "df = pd.DataFrame.from_dict({'lab':['Min','Max'], 'val':[np.mean(c_samp['Min']), np.mean(c_samp['Max'])]})\n",
    "errors = [np.std(c_samp['Min']), np.std(c_samp['Max'])] \n",
    "ax = df.plot.bar(x='lab', y='val', yerr=errors, rot=0)\n",
    "ax.set_ylabel('Average value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remarks**: \n",
    "* Please answer all of the following questions within the notebook itself. Remember that the only files you will submit are the notebook and the fully-implemented `.py` files.\n",
    "* Do not change the notebook's cells unless you were specificly told to (such as the \"Answers\" cells etc.). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions:\n",
    "**Q1:** What information can you get from histograms and what information can you get from boxplots? {5 points}\n",
    "\n",
    "**Q2:** Which of the visualizations is less informative? What parameter of empirical histogram can mislead you if not chosen wisely? {4 points}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answers:\n",
    "**Q1:**\n",
    "\n",
    "\n",
    "**Q2:**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have visualized  the data, cleaned them and obtained some insights from them, we would like to compute the summary statistics for each feature. Implement the `sum_stat` function which returnes a dictionary of dictionaries, meaning that a key value of a feature will return a dictionary with keys of min, Q1, median, Q3, max. **Do not use pandas `describe()` function in this implementation**.\n",
    "It should look something like this:\n",
    "\n",
    "d_summary = {<br>\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;\"MSTV\": {<br>\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;\"min\": 2.0, <br>\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;\"Q1\": 3.0,<br>\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;\"median\": 4.0, <br>\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;\"Q3\": 5.0, <br>\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;\"max\": 6.0, <br>\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;},<br>\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;\"LB\": {<br>\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;\"min\": ..., <br>\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;\"Q1\": ...,<br>\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;\"median\": ..., <br>\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;\"Q3\": ..., <br>\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;\"max\": ..., <br>\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;},<br>\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;<br>\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;}<br>\n",
    "<br> \n",
    "You can access with : d_summary[\"MSTV\"][\"min\"] = 2.0\n",
    "\n",
    "You can use that output in order to have another cleanup and this time, it will be a cleanup of outliers. We will stick to the definition of an outlier according to the five number summary that are actually represented by boxplots. Just as a reminder and comparison to a normal distribution, have a look at the next figure:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1400/1*NRlqiZGQdsIyAu0KzP7LaQ.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from clean_data import sum_stat as sst\n",
    "d_summary = sst(c_samp)\n",
    "print(d_summary['MSTV'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the function `rm_outlier` that will have the output of `sum_stat` as an input and will return a DataFrame of the that will have outliers removed. In order to avoid issues of different lengths of entries, instead of removing the outliers, simply replace them with `np.nan`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from clean_data import rm_outlier\n",
    "c_no_outlier = rm_outlier(c_samp, d_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take the features `Median`, `Mean` and `Mode` for comparioson previous to outliers removal and after it using boxplots. First we plot the original data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_samp.boxplot(column=['Median','Mean','Mode'])\n",
    "plt.ylabel('Fetal Heart Rate [bpm]')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we plot the \"clean data\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_no_outlier.boxplot(column=['Median','Mean','Mode'])\n",
    "plt.ylabel('Fetal Heart Rate [bpm]')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions:\n",
    "**Q3:** Boxplotting the data after the outliers were removed shows us that there are no outliers anymore. Is it necessarily always the case, meaning if you take the \"clean\" data and boxplot it again will it have no outliers for sure? {4 points}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answers:\n",
    "**Q3:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, there is one more thing that should be reminded in the aspect of data exploration and it is the second rule of this field that states the following: **USE COMMON SENSE!**\n",
    "\n",
    "What it really means is that if you have some physiological prior (e.g. you know the range of values of your features), so you should have some sanity checks. For example, the feature `LB` tells us about the FHR baseline. It won't make any sense if we found values that are higher than 500 bpm, not even mentioning non-positive values. Your next mission is to implement the function `phys_prior` where you choose one feature (which is not `LB`), explain what you think it's normal range is and clean it accordingly. The function will have `c_samp`, the feature that you chose and a threshold as inputs. The explanation should be written shortly as a comment next to the input as you can see at the following cell. The only lines you can change here are the `feature` value, the `thresh` value and its comment. We will accept any reasonable explanation and a corresponding threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from clean_data import phys_prior as phpr\n",
    "feature = 'LB'\n",
    "'''\n",
    "FHR is normally between 120 bpm to 160 bpm, so most of the cases the mean FHR will be in that range. \n",
    "But, some of the time we have accelerations and than the FHR is in the range of 160-240, \n",
    "so finally we will not expect to receive mean over 240.\n",
    "'''\n",
    "thresh = 240\n",
    "filt_feature = phpr(c_samp, feature, thresh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Scaling: Standardization and Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By this point you have successfully cleaned your dataset, well done! The clean dataset was saved in pickle format called `objs.pkl`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('objs.pkl', 'rb') as f:\n",
    "    CTG_features, CTG_morph, fetal_state = pickle.load(f)\n",
    "orig_feat = CTG_features.columns.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will address an important step in data science which is called feature scaling. Here we will discuss about standardization and normalization. As you saw in the lectures, scaling enables us to prepare features that take their value in different ranges and map them to a “normalized” features that take their values in similar ranges.\n",
    "Implement the methods of the class `NSD`. The `transform` method function will return the data normalized/standardized according to the mode. You should also choose two features for comparison (using histograms) between the original data and the different modes. The two histograms should be plotted at the same figure with different colors as you saw in the lecture. You should have one figure per `mode` plus original data so ath the next cell you would end up with 4 figures having two histograms per figure. Use `matplotlib` as you saw in your tutorials. The argument `flag` is used for visibility of histograms.  There are three types of `mode`: `'standard','MinMax' and 'mean'` as shown in the lecture. You may change `selected_feat` in the next cell if youl'd like to see that your implemetations apply well on other features as well.\n",
    "\n",
    "\n",
    "**Notice:**\n",
    "\n",
    "1. `fit` method should calculate all needed values for all possible modes. The mode is chosen only in `transform` (or `fit_transform`).\n",
    "\n",
    "\n",
    "2. Once you apply the transformation, it should return a datframe which all of its samples in all of of the features are scaled correctly according to the mode. The chioce of `selected_feat` is only for those features that you would like to plot their histograms but all of the features have to be scaled regardless the ones you choose to plot.\n",
    "**Hint**: Use of `df.apply(np.mean)` for example would return a series that could be broadcasted easily with df1 for different calculations. Thus, `df - df.apply(np.mean)` would return a Dataframe where the **adequate** mean value was extracted, for instance the `LB` series will now be `LB-mean(LB)`. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from clean_data import NSD\n",
    "selected_feat = ('LB','ASTV')\n",
    "scaler = NSD()\n",
    "orig = scaler.fit_transform(CTG_features, selected_feat=selected_feat, flag=True)\n",
    "nsd_std = scaler.fit_transform(CTG_features, mode='standard', selected_feat=selected_feat, flag=True)\n",
    "nsd_norm = scaler.fit_transform(CTG_features, mode='MinMax', selected_feat=selected_feat, flag=True)\n",
    "nsd_norm_mean = scaler.fit_transform(CTG_features, mode='mean', selected_feat=selected_feat, flag=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions:\n",
    "**Q4:** Explain why normalization is not useful when there are outliers with extremely large or small values. {5 points}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answers:\n",
    "**Q4:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part II: Linear Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, after all of the hard work we can now harvest the fruits (your functions from Part I) in order to do some proper machine learning!  \n",
    "\n",
    "Note: It is recommended that you attend the second workshop for this part and use the notes in your homework folder.\n",
    "\n",
    "In this assignment we will assume that our data is linearly separable and we will use logistic regression as our classification method. In other words, we choose a linear hypothesis class function . We would try to make the separation in the feature domain (i.e. our graph axes are the features) and we will have a multiclass problem.\n",
    "\n",
    "For every sample (example as called in the lecture) we have two types of labels and we will deal only with one of them. Our goal is to learn the function that gets a sample as an input and returns a predicted value which is supposed to be the calss (label) that it belongs to. Our type of label will be `fetal_state`. Before we continue towards the \"learning\" part we will have another look at our data. Starting with our labels distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "# with seaborn:\n",
    "g = sns.countplot(x = 'NSP', data = fetal_state, ax=axes[0])\n",
    "g.set(xticklabels=['Normal','Suspect','Pathology'])\n",
    "# with matplotlib\n",
    "fetal_state.NSP.value_counts().plot(kind=\"pie\", labels=['Normal','Suspect', 'Patholgy'], autopct='%1.1f%%', ax=axes[1])\n",
    "plt.show()\n",
    "idx_1 = (fetal_state == 1).index[(fetal_state == 1)['NSP'] == True].tolist()\n",
    "idx_2 = (fetal_state == 2).index[(fetal_state == 2)['NSP'] == True].tolist()\n",
    "idx_3 = (fetal_state == 3).index[(fetal_state == 3)['NSP'] == True].tolist()\n",
    "print(\"Normal samples account for \" + str(\"{0:.2f}\".format(100*len(idx_1)/len(fetal_state))) + \"% of the data.\")\n",
    "print(\"Suspect samples account for \" + str(\"{0:.2f}\".format(100*len(idx_2)/len(fetal_state))) + \"% of the data.\")\n",
    "print(\"Pathology samples account for \" + str(\"{0:.2f}\".format(100*len(idx_3)/len(fetal_state))) + \"% of the data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that most of the CTG's were labeled as normal. Mostly, lables are made by professionals (in our case, doctors) based on the interpretation of the FHR and our job is to make the computer make the same decisions as a doctor would do but automatically. Now let's get the feeling of how well the features correlate with the labels and with one another. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = 100\n",
    "feat = 'Width'\n",
    "plt.hist(CTG_features[feat].loc[idx_1], bins, density=True, alpha=0.5, label='Normal')\n",
    "plt.hist(CTG_features[feat].loc[idx_2], bins, density=True, alpha=0.5, label='Suspect')\n",
    "plt.hist(CTG_features[feat].loc[idx_3], bins, density=True, alpha=0.5, label='Pathology')\n",
    "plt.xlabel('Histogram Width')\n",
    "plt.ylabel('Probabilty')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.plotting.scatter_matrix(CTG_features[['LB','AC','FM','UC']])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions:\n",
    "**Q5:** What information does feature-feature correlation provide? {4 points}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answers:\n",
    "**Q5:**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, we are pretty much done with data exploration. Now the learning part begins. As you saw in the tutorials, one of the most common and useful packages that Python has to offer in the learning field is `sklearn` package. The first thing you need to do after exploring your data is to divide it into 2 sets: `training set` and `test set`. As a rule of thumb, a typical split of your dataset is 80%, 20%  respectively. For simplicity, in this exercise we will use only two splits - train and test, however the correct way is to split the dataset to 3 splits - train, validation and test where the validation set is used to tune your model, and the test set should be used only once to report the final results.\n",
    "\n",
    "\n",
    "One of the most common linear classification models is logistic regression – abbreviated ‘LR’. We will use this model through our assignment from now on. \n",
    "Implement the function `pred_log` which is in the module `lin_classifier`. It should return a tuple of two elements. The first one is a vector of predicted classes and the other one is the weighting matrix (w) that should have the shape of (# of classes, # of features).\n",
    "\n",
    "As you noticed, most of our data is labeled as \"Normal\" which means that our data is *imbalanced*. This is the reason why we *stratification* when we split out data to training and test sets. Stratification means that the split preserves the percentage of samples for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from lin_classifier import *\n",
    "orig_feat = CTG_features.columns.values\n",
    "X_train, X_test, y_train, y_test = train_test_split(CTG_features, np.ravel(fetal_state), test_size=0.2, \n",
    "                                                    random_state=0, stratify=np.ravel(fetal_state))\n",
    "logreg = LogisticRegression(solver='saga', multi_class='multinomial', penalty='none', max_iter=10000)\n",
    "y_pred, w = pred_log(logreg, X_train, y_train, X_test)\n",
    "print(\"Accuracy is: \" + str(\"{0:.2f}\".format(100 * metrics.accuracy_score(y_test, y_pred))) + \"%\")\n",
    "print(\"F1 score is: \" + str(\"{0:.2f}\".format(100 * metrics.f1_score(y_test, y_pred, average='macro'))) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully, you got about 88% accuracy. Not bad! This is a better result comparing the naive classifier (prediciting a \"Normal\" label for every sample) that should have given us around 77% accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Now let's see if normalization and standardization help us. Fill the next cell and print the accuracy and F1 of the model on the testing set after standardizng and training correctly. *Important notes*:\n",
    "\n",
    "* Avoid information leakage!\n",
    "* Do not apply the `NSD` methods on the labels. \n",
    "* Set the `flag` argument to `False` when using `NSD` methods.\n",
    "* {3 points}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------Implement your code here:------------------------------\n",
    "\n",
    "# --------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now choose one of the modes and stick to it. Let's visualize our learned parameters. Use your chosen weight matrix as an input to the function `w_no_p_table` in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change input_mat to your weight matrix from the cell above\n",
    "w_no_p_table(input_mat,orig_feat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions:\n",
    "**Q6:** Mention one advantage and one limitation of using cross entropy. {5 points}\n",
    "\n",
    "\n",
    "**Q7:** By selecting one feature at a time and compare their learned weights by looking at plots we had, what can you tell about the weights relations? Why does it happen? **Hint:** notice the sign of the weights and remember that an exponent is a monotonic function. {4 points}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answers:\n",
    "**Q6:**\n",
    "\n",
    "**Q7:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now let's recall that in the lecture you saw that accuracy is not always our best measure. Sensitivity and specificity can be much more informative and important mostly. The choise to train a model to have better results in sensitivity aspect rather than specificty or vice versa depends on our application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnf_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
    "ax = plt.subplot()\n",
    "sns.heatmap(cnf_matrix, annot=True, xticklabels=['Normal','Suspect','Pathology'], yticklabels=['Normal','Suspect','Pathology'])\n",
    "ax.set(ylabel='True labels', xlabel='Predicted labels')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions:\n",
    "**Q8:** What do you think is more important to us with this data? What is the clinical risk/cost for False Positive (FP) and False Negative (FN)? {3 points}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answers:\n",
    "**Q8:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, now we will try to handle one of the main issues in learning which is called **overfitting** and one way to deal with it is called **regularization**.\n",
    "\n",
    "There are several types of regularizations and in this assignment we will expirience two of them:\n",
    "\n",
    "1) Loss regularization.\n",
    "\n",
    "2) Validation.\n",
    "\n",
    "The loss function is a function that takes the predicted values and the labels and *measures* how \"close\" they are to each other. Our demand is that this \"distance\" (metric) would be as low as possible. In addition to it, we can add more \"demands\" that we can represent by mathematical terms. For example, we can demand that the number of coefficients won't get to large or we can try to restrict their values. A more physical example is a demand that our signal has to be smooth. When we try to minimize the new loss function we actually try to find the result which is the comprimise of our demands. We can also \"favour\" one demand over another using **regularization parameters**.\n",
    "\n",
    "You saw in the lecture \"demands\" on the learned weights and represented those demands mathematically using $ L_1 $ and $ L_2 $ norms. The regularization parameter was denoted as $\\lambda$ (please notice that sometimes it is common to use the notation of $ c $ where $\\lambda = c^{-1}$). Now it's your turn to become artists! Change and/or add arguments to *LogisticRegression* class in the next cell and perform learning using two regularizations: $ L_1 $ and $ L_2 $. Examine your results using the confusion matrix as we did before. Tune your regularization parameter until you get a result that you think is reasonable and that it brings the sensitivity/specificity (depending on what you chose before) to the maximum.\n",
    "\n",
    "Use the definition of sensitivity/specificity as defined in the lecture.\n",
    "{8 points}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#--------------------------------------------- Implement your code here:----------------------------------------------------------\n",
    "\n",
    "#----------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you are happy with your results, let's compare the coefficients of the two norms. Choose two weightning matrices (one calculated using $ L_2 $ and the other calculated using $ L_1 $) and use them as inputs in `w_all_tbl` function. This function sorts the weights according to their $ L_2 $ norm (so the first argument has to be the matrix of $ L_2 $) and compares them to $ L_1 $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_all_tbl(w2, w1, orig_feat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the features are ordered differently because they are sorted according to $ L_2 $ values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions:\n",
    "**Q9:** What is the difference that you can see when plotting $ L_1 $ vs. $ L_2 $? Could you expect it ahead? {4 points}\n",
    "\n",
    "**Q10:** Why is LASSO coupled with feature selection? {5 points}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answers:\n",
    "**Q9:**\n",
    "\n",
    "\n",
    "**Q10:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will use a method that help us to choose what we call *hyperparameters* of the model. This is also a method of regularization and it is called **validation**. There are several types of validation and here we will use *stratified K-fold cross validation*. The hyperparmeters that we would like to choose are the norms that we want to train with and the regularization parameter. Again, we use stratification for the folds to prevent biased learning.\n",
    "\n",
    "Implement the function `cv_kfold` in `lin_classifier` module. We will use `X_train` as our training set that will be iteratively divided into $ K-1 $ training sets and one validation set. **Notice:** choose wisely where to apply `NSD` fit and transformation to avoid information leakage in every iteration. In this function you should build a list of dictionaries called `validation_dict` where each element in the list contains a dictionary with 4 keys name: `C, penalty, mu and sigma`. For every pair of parameters (`C and penalty`) you will run $ K $ validations and `mu and sigma` will be calculated as the average loss and standard deviation over $ K $ folds respectively. Use the function `log_loss` from `sklearn.metrics` that was already imported in `lin_classifier`. One more thing, you will have to implement a simple modification to `pred_log` function using the `flag` argument. When this flag is set to `True`, the function should return the probabilities of the classes and not the classes themselves. This is the output that `log_loss` function expects to get.\n",
    "In the next cell, build a list called `C` that has up to 6 regularization parameters (different lambda). Set `K` as number of folds. Choose `mode` for `NSD` and set `penalty = ['l1,'l2]`. Then, excecute the function `cv_kfold` which should return the list of dictionaries named `val_dict` that will be used in the following cell.\n",
    "This function might take a while to perform depending on $ K $ and the number of regularization parameters you will choose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------Implement your code here:----------------------------------------\n",
    "\n",
    "#------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize your results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "\n",
    "cm = plt.get_cmap('nipy_spectral')\n",
    "for i, d in enumerate(val_dict):\n",
    "    x = np.linspace(0, d['mu'] + 3 * d['sigma'], 1000)\n",
    "    ax = plt.plot(x,stats.norm.pdf(x, d['mu'], d['sigma']), label=\"p = \" + d['penalty'] + \", C = \" + str(d['C']), color=cm(i*20))\n",
    "    plt.title('Gaussian distribution of the loss')\n",
    "    plt.xlabel('Average loss')\n",
    "    plt.ylabel('Probabilty density')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now choose parameters according to the results. Explain why did you choose these hyperparmeters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can train you model with the **full training set**. {5 points}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------Implement your code here:----------------------------------------\n",
    "\n",
    "#------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great job! So as you can see results did get better but not by that much but you got the feeling how to handle with data, what are the basics of learning and what are the real effects and applications of what you saw in the lectures. Now, one last thing: A possible reason for the poor improvements is that our data is probably not linearly separable and we used a linear classifier. There are two basic approaches for this kind of problem:\n",
    "The first one is to use non-linear classifier and the second one is to perform a transformation on our data so it will become linearly separable in another space. Here is an example of 2D data that can also visualize the problem and the second approach:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://sthalles.github.io/assets/fisher-ld/feature_transformation.png\" width=600 align=\"center\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the features were non-linearly transformed simply by squaring each feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions:\n",
    "**Q11:** Look at the data. Why was it reasonable to expect that squaring each feature would make the data linearly sperable? {2 points}\n",
    "\n",
    "**Q12:** Sugest another non-linear transformation (where **both** new axes are a function of **both** $(x_1,x_2)$ and they are not equal) that would make the data linearly seperable so that the line that seperates the two data types will be perpendicular to one of the new axes. Write the new two features (axes) **explicitly** as a funciton of $ (x_1,x_2) $. Use LaTex to write mathematical operations. {5 points}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answers:\n",
    "**Q11:** \n",
    "\n",
    "**Q12:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, just to get the feeling of better results when we go non-linear, let's try the random forest classifier. All you have to do is just choose one of the modes of the `NSD` tranformation method and see if you got better results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier as rfc\n",
    "mode = 'MinMax' # choose your method\n",
    "clf = rfc(n_estimators=10)\n",
    "clf.fit(scaler.fit_transform(X_train, mode=mode), y_train)\n",
    "y_pred = clf.predict(scaler.transform(X_test, mode=mode))\n",
    "cnf_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
    "ax = plt.subplot()\n",
    "sns.heatmap(cnf_matrix, annot=True, xticklabels=['Normal', 'Suspect', 'Pathology'],\n",
    "            yticklabels=['Normal', 'Suspect', 'Pathology'])\n",
    "ax.set(ylabel='True labels', xlabel='Predicted labels')\n",
    "plt.show()\n",
    "print(\"Accuracy is: \" + str(\"{0:.2f}\".format(100 * metrics.accuracy_score(y_test, y_pred))) + \"%\")\n",
    "print(\"F1 score is: \" + str(\"{0:.2f}\".format(100 * metrics.f1_score(y_test, y_pred, average='macro'))) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's all folks!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
